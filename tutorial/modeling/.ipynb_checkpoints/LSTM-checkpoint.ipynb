{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a237b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE 적용\n",
    "# 원핫인코딩 적용 안함\n",
    "# Y transform 안함\n",
    "# 결과가 나오는데 반올림하면 전부 유지 그래서 원핫 인코딩을 하는 게 좋을 것 같다.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6e75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일을 읽어옴\n",
    "data = pd.read_excel('./modeling_data/2weeks_summary3.xlsx')\n",
    "\n",
    "# MinMaxScaler()로 정규화\n",
    "scaler = MinMaxScaler()\n",
    "data[['Height', 'Weight', 'Step', 'Burn', 'Eat', 'Sleep']] = scaler.fit_transform(data[['Height', 'Weight', 'Step', 'Burn', 'Eat', 'Sleep']])\n",
    "\n",
    "\n",
    "# Y_real 에는 마지막 원래 라벨값 넣어둠 -> 예측된 값과 비교하기 위해 쓰임\n",
    "Y_real = data[['Label']]\n",
    "Y_real = Y_real.to_numpy()\n",
    "\n",
    "# 이유는 모르겠는데 라벨 타입을 인트로 바꿈 (에러 해결하기 위해 이랬음)\n",
    "X = data[['Height','Weight','Step','Burn','Eat','Sleep']].values\n",
    "data = data.astype({'Label':'int'})\n",
    "Y = data[['Label']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84340e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "input_size = 6        # input_size, 입력 변수의 개수\n",
    "print(input_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# seq_length 만큼 데이터를 묶어주는 함수임\n",
    "def seq_data(x, y, sequence_length):\n",
    "    x_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(x) - sequence_length):\n",
    "        x_seq.append(x[i:i + sequence_length])\n",
    "        y_seq.append(y[i + sequence_length])\n",
    "\n",
    "    return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(y_seq).to(device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6036d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of test_y: torch.Size([574, 3]) \n",
      "\n",
      "Before SMOTE OverSampling, the shape of x: torch.Size([1926, 6, 6])\n",
      "Before SMOTE OverSampling, the shape of y: torch.Size([1926, 1]) \n",
      "\n",
      "After SMOTE OverSampling, the shape of x: (3672, 36)\n",
      "After SMOTE OverSampling, the shape of y: (3672,) \n",
      "\n",
      "After SMOTE OverSampling, the shape of x: torch.Size([3672, 6, 6])\n",
      "After SMOTE OverSampling, the shape of y: torch.Size([3672, 3]) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lab01\\AppData\\Local\\Temp\\ipykernel_3328\\2444376344.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  return torch.FloatTensor(x_seq).to(device), torch.FloatTensor(y_seq).to(device).view(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "#split = 1600           # split 개수 만큼 train이 된다\n",
    "split = int(0.7 * len(X))\n",
    "epochs = 500\n",
    "sequence_length = 6    # 함께 묵을 날짜 수\n",
    "batch_size =6\n",
    "# seq_data() 함수를 통해 데이터를 묶은 후 각각 x_seq, y_seq에 넣어 줌\n",
    "x_seq, y_seq = seq_data(X, Y, sequence_length)\n",
    "\n",
    "\n",
    "# train / test로 쪼갬\n",
    "x_train_seq = x_seq[:split]\n",
    "y_train_seq = y_seq[:split]\n",
    "x_test_seq = x_seq[split:]\n",
    "y_test_seq = y_seq[split:]\n",
    "\n",
    "# SMOTE 사용하기 위해 차원을 바꿔 줌\n",
    "nsamples, nx, ny = x_train_seq.shape\n",
    "# print(\"nsamples :\", nsamples)\n",
    "# print(\"nx :\", nx)\n",
    "# print(\"ny :\", ny)\n",
    "\n",
    "x_train_seq = x_train_seq.reshape((nsamples, nx*ny))\n",
    "\n",
    "#y_smote = torch.Tensor(pd.get_dummies(y_smote).values)\n",
    "y_test_seq = y_test_seq.flatten()\n",
    "y_test_seq = torch.Tensor(pd.get_dummies(y_test_seq).values)\n",
    "#y_test_seq = y_test_seq.tolist()\n",
    "#y_test_seq = torch.Tensor(pd.get_dummies(y_test_seq).values)\n",
    "print('The shape of test_y: {} \\n'.format(y_test_seq.shape))    # train만 SMOTE 적용 (y)\n",
    "\n",
    "#test_y 원핫인코딩 확인용\n",
    "#print(y_test_seq)\n",
    "\n",
    "\n",
    "# SOMTE 적용 전 shape 확인용\n",
    "print('Before SMOTE OverSampling, the shape of x: {}'.format(x_seq.shape))\n",
    "print('Before SMOTE OverSampling, the shape of y: {} \\n'.format(y_seq.shape))\n",
    "\n",
    "# SMOTE 기법 사용하여 train 늘려 줌\n",
    "sm = SMOTE(random_state=0)\n",
    "x_smote, y_smote = sm.fit_resample(x_train_seq, y_train_seq)\n",
    "\n",
    "# SMOTE 적용 후 shape 확인용\n",
    "print('After SMOTE OverSampling, the shape of x: {}'.format(x_smote.shape))       # train만 SMOTE 적용 (x)\n",
    "print('After SMOTE OverSampling, the shape of y: {} \\n'.format(y_smote.shape))    # train만 SMOTE 적용 (y)\n",
    "\n",
    "x_smote = torch.Tensor(x_smote)\n",
    "tempx, tempx2  = x_smote.shape\n",
    "x_smote = x_smote.reshape((tempx, sequence_length, input_size))\n",
    "\n",
    "y_smote = torch.Tensor(pd.get_dummies(y_smote).values)\n",
    "#print(y_smote)\n",
    "#tempy, = y_smote.shape\n",
    "#y_smote = y_smote.reshape((tempy, 1))\n",
    "\n",
    "print('After SMOTE OverSampling, the shape of x: {}'.format(x_smote.shape))       # train만 SMOTE 적용 (x)\n",
    "print('After SMOTE OverSampling, the shape of y: {} \\n'.format(y_smote.shape))    # train만 SMOTE 적용 (y)\n",
    "\n",
    "#y_smote= pd.get_dummies(y_smote)\n",
    "#print(y_smote)\n",
    "\n",
    "####################################\n",
    "# Before SMOTE OverSampling, the shape of x: torch.Size([1926, 6, 6])\n",
    "# Before SMOTE OverSampling, the shape of y: torch.Size([1926, 1]\n",
    "# After SMOTE OverSampling, the shape of x: torch.Size([4347, 6, 6])\n",
    "# After SMOTE OverSampling, the shape of y: torch.Size([4347, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d76c80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset에 넣어줌\n",
    "train = torch.utils.data.TensorDataset(x_smote, y_smote)\n",
    "test = torch.utils.data.TensorDataset(x_test_seq, y_test_seq)\n",
    "\n",
    "\n",
    "# 길이 확인용 train은 smote 한 후 길이가 늘어나 있고\n",
    "# test는 smote를 적용안해서 전체 길이 - seq_length가 들어가 있음\n",
    "# print(\"len(train): \", len(train))\n",
    "# print(\"len(test): \", len(test))\n",
    "\n",
    "##################################\n",
    "#len(train):  4347\n",
    "#len(test):  326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5683218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader 와 testloader에 각각 train과 test셋을 넣어 줌\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train, batch_size=6, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=test, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc7f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(trainloader.dataset))\n",
    "\n",
    "num_layers = 1  # lstm 층의 수, Number of recurrent layers\n",
    "# setting num_layers=2 would mean stacking two LSTMs together to form stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results\n",
    "hidden_size = 6  # 은닉층의 피처 개수\n",
    "num_classes = 3  # number of output classes\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_size, sequence_length, num_layers, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.device = device\n",
    "        self.input_size = input_size  # input size\n",
    "        self.num_classes = num_classes  # number of classes\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "        self.sequence_length = sequence_length  # sequence length\n",
    "        self.num_layers = num_layers  # number of layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)  # lstm\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes)  # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device)\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size)  # reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)  # first Dense\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc(out)  # Final Output\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7bcae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "model = LSTM(input_size=input_size, num_classes=num_classes, hidden_size=hidden_size, num_layers=num_layers,\n",
    "             sequence_length=sequence_length, device=device).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()  # MSE 손실함수 사용\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Adam optimizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012af829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : %d loss : %.4f, accuracy :   (0, 0.21771999928199387, 0.4127774238586426)\n",
      "Epoch : %d loss : %.4f, accuracy :   (1, 0.2127135843769008, 0.43555524945259094)\n",
      "Epoch : %d loss : %.4f, accuracy :   (2, 0.21084885642516846, 0.4502773880958557)\n",
      "Epoch : %d loss : %.4f, accuracy :   (3, 0.2066177043309009, 0.4780552089214325)\n",
      "Epoch : %d loss : %.4f, accuracy :   (4, 0.2007147090180832, 0.519999623298645)\n",
      "Epoch : %d loss : %.4f, accuracy :   (5, 0.19710022044288955, 0.5316663980484009)\n",
      "Epoch : %d loss : %.4f, accuracy :   (6, 0.1924315105661068, 0.5541661381721497)\n",
      "Epoch : %d loss : %.4f, accuracy :   (7, 0.1868717919645551, 0.5838884711265564)\n",
      "Epoch : %d loss : %.4f, accuracy :   (8, 0.18015409230552856, 0.5919439792633057)\n",
      "Epoch : %d loss : %.4f, accuracy :   (9, 0.175578473056697, 0.6066664457321167)\n",
      "Epoch : %d loss : %.4f, accuracy :   (10, 0.1709906698928939, 0.6199995279312134)\n",
      "Epoch : %d loss : %.4f, accuracy :   (11, 0.16769567803400598, 0.6377770900726318)\n",
      "Epoch : %d loss : %.4f, accuracy :   (12, 0.16446527934274058, 0.6438885927200317)\n",
      "Epoch : %d loss : %.4f, accuracy :   (13, 0.1625679781770005, 0.657499372959137)\n",
      "Epoch : %d loss : %.4f, accuracy :   (14, 0.15935220496935976, 0.6647217273712158)\n",
      "Epoch : %d loss : %.4f, accuracy :   (15, 0.15739630540114602, 0.6711105704307556)\n",
      "Epoch : %d loss : %.4f, accuracy :   (16, 0.1554835907128804, 0.6888887882232666)\n",
      "Epoch : %d loss : %.4f, accuracy :   (17, 0.15341696388459478, 0.6930546760559082)\n",
      "Epoch : %d loss : %.4f, accuracy :   (18, 0.15216948929869759, 0.6969441771507263)\n",
      "Epoch : %d loss : %.4f, accuracy :   (19, 0.14919884629194546, 0.7044443488121033)\n",
      "Epoch : %d loss : %.4f, accuracy :   (20, 0.1483020178267574, 0.7074992060661316)\n",
      "Epoch : %d loss : %.4f, accuracy :   (21, 0.14637041416360078, 0.7155550122261047)\n",
      "Epoch : %d loss : %.4f, accuracy :   (22, 0.14538797685949534, 0.7188881635665894)\n",
      "Epoch : %d loss : %.4f, accuracy :   (23, 0.14377328449421944, 0.7227774262428284)\n",
      "Epoch : %d loss : %.4f, accuracy :   (24, 0.1419994960420345, 0.7244440317153931)\n",
      "Epoch : %d loss : %.4f, accuracy :   (25, 0.14130862426694507, 0.724999725818634)\n",
      "Epoch : %d loss : %.4f, accuracy :   (26, 0.14017639942832438, 0.731665849685669)\n",
      "Epoch : %d loss : %.4f, accuracy :   (27, 0.1392475975421714, 0.7322221994400024)\n",
      "Epoch : %d loss : %.4f, accuracy :   (28, 0.13724773794144782, 0.7399995923042297)\n",
      "Epoch : %d loss : %.4f, accuracy :   (29, 0.13620329802715847, 0.7380548119544983)\n",
      "Epoch : %d loss : %.4f, accuracy :   (30, 0.13491286656034052, 0.7463885545730591)\n",
      "Epoch : %d loss : %.4f, accuracy :   (31, 0.13450057943826357, 0.7474990487098694)\n",
      "Epoch : %d loss : %.4f, accuracy :   (32, 0.13343197781154337, 0.7444438934326172)\n",
      "Epoch : %d loss : %.4f, accuracy :   (33, 0.13346465932451845, 0.74638831615448)\n",
      "Epoch : %d loss : %.4f, accuracy :   (34, 0.1315686277606908, 0.7574999332427979)\n",
      "Epoch : %d loss : %.4f, accuracy :   (35, 0.13084939768190915, 0.7572214007377625)\n",
      "Epoch : %d loss : %.4f, accuracy :   (36, 0.13005332855258162, 0.7605551481246948)\n",
      "Epoch : %d loss : %.4f, accuracy :   (37, 0.1294897741796388, 0.7588886022567749)\n",
      "Epoch : %d loss : %.4f, accuracy :   (38, 0.12825827336310122, 0.7672216296195984)\n",
      "Epoch : %d loss : %.4f, accuracy :   (39, 0.12795442131536652, 0.7636106610298157)\n",
      "Epoch : %d loss : %.4f, accuracy :   (40, 0.12628544646838766, 0.7708330154418945)\n",
      "Epoch : %d loss : %.4f, accuracy :   (41, 0.12652228962404813, 0.7686103582382202)\n",
      "Epoch : %d loss : %.4f, accuracy :   (42, 0.12498384344793272, 0.7730555534362793)\n",
      "Epoch : %d loss : %.4f, accuracy :   (43, 0.12356626385749847, 0.7752774953842163)\n",
      "Epoch : %d loss : %.4f, accuracy :   (44, 0.12392348792476982, 0.7683333158493042)\n",
      "Epoch : %d loss : %.4f, accuracy :   (45, 0.1226946019173411, 0.7733325958251953)\n",
      "Epoch : %d loss : %.4f, accuracy :   (46, 0.12165416527868193, 0.7786112427711487)\n",
      "Epoch : %d loss : %.4f, accuracy :   (47, 0.12034358880086954, 0.7877776622772217)\n",
      "Epoch : %d loss : %.4f, accuracy :   (48, 0.11969665239650293, 0.7819440960884094)\n",
      "Epoch : %d loss : %.4f, accuracy :   (49, 0.11845062214126384, 0.7877776622772217)\n",
      "Epoch : %d loss : %.4f, accuracy :   (50, 0.11765151039934625, 0.7930551767349243)\n",
      "Epoch : %d loss : %.4f, accuracy :   (51, 0.11864829561338315, 0.7866665720939636)\n",
      "Epoch : %d loss : %.4f, accuracy :   (52, 0.11622675511874928, 0.7891665697097778)\n",
      "Epoch : %d loss : %.4f, accuracy :   (53, 0.11620607424460448, 0.7902778387069702)\n",
      "Epoch : %d loss : %.4f, accuracy :   (54, 0.11476068566150026, 0.7974995970726013)\n",
      "Epoch : %d loss : %.4f, accuracy :   (55, 0.11428011813295359, 0.8008335828781128)\n",
      "Epoch : %d loss : %.4f, accuracy :   (56, 0.11348080774769187, 0.795833170413971)\n",
      "Epoch : %d loss : %.4f, accuracy :   (57, 0.11276877318429791, 0.7991665601730347)\n",
      "Epoch : %d loss : %.4f, accuracy :   (58, 0.11224315104111299, 0.8047223687171936)\n",
      "Epoch : %d loss : %.4f, accuracy :   (59, 0.11195424064808203, 0.8024996519088745)\n",
      "Epoch : %d loss : %.4f, accuracy :   (60, 0.11040023794349015, 0.8044444918632507)\n"
     ]
    }
   ],
   "source": [
    "n = len(trainloader)    # n에 trainloader 길이 넣어 줌\n",
    "loss_graph = []         # 손실값 구하는 데 사용할 배열\n",
    "accuracy_graph = []\n",
    "\n",
    "# epoch 만큼 반복하며 loss 구하며 최적화\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    corr = 0.0\n",
    "    for data in trainloader:\n",
    "        seq, target = data\n",
    "#         print(seq.shape)  #6*6*3\n",
    "#         print(target.shape)   #6*3\n",
    "        outputs = model(seq)   # model.forward()랑 그냥이랑 무슨차이 -> 그냥 model이 더 좋은듯\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        out = model(seq)\n",
    "#         out=out.detach().numpy()\n",
    "#         print(type(out))\n",
    "       \n",
    "#         target=target.detach().numpy()\n",
    "#         print(target)\n",
    "#         print(out)\n",
    "\n",
    "        target = torch.argmax(target, dim=1)\n",
    "        out = torch.argmax(out, dim=1)\n",
    "#         print(\"1\",target)\n",
    "#         print(\"2\",out)\n",
    "        \n",
    "        corr += (target == out).sum()/3600\n",
    "        \n",
    "        \n",
    "    #if epoch % 100 == 0:\n",
    "    print(\"Epoch : %d loss : %.4f, accuracy :  \"  ,(epoch, running_loss / n,corr.item() ))\n",
    "    accuracy_graph.append(corr.item())\n",
    "    loss_graph.append(running_loss / n)\n",
    "    \n",
    "concatdata = torch.utils.data.ConcatDataset([test])\n",
    "data_loader = torch.utils.data.DataLoader(concatdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://076923.github.io/posts/Python-pytorch-9/ 참고하면 테스트 데이터 정의해서 모델 평가하는 것도 확인 가능 -> 입력과 차원을 같게\n",
    "with torch.no_grad():   # gradient 옵션을 그만할 때 사용, 보통 더이상 학습 안하고 학습된 모델로 결과를 볼 때 사용\n",
    "    pred = []\n",
    "    model.eval()        # evaluation 과정에서 사용하지 않아도 되는 layer들을 off 시켜줌\n",
    "    for data in data_loader:\n",
    "        seq, target = data\n",
    "        out = model(seq)\n",
    "        pred += out.cpu().tolist()\n",
    "\n",
    "print(pred)        \n",
    "        \n",
    "pred = torch.Tensor(pred)  #이유는 모르겠는데 pred가 list라 argmax 쓰기 위해 torch로 변환\n",
    "# 변환 후 결과 확인(Tensor)\n",
    "\n",
    "length = len(pred)\n",
    "pred_result = torch.argmax(pred, dim=1)\n",
    "\n",
    "# 0~2가 리턴, 라벨을 1~3으로해서 하나씩 더해야함\n",
    "pred_result = pred_result.tolist()\n",
    "print(pred_result)\n",
    "\n",
    "#라벨링 결과 확인\n",
    "print(pred_result)\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "forTestLength = len(X)-sequence_length-split\n",
    "#print(\"Num of Test : \", forTestLength)\n",
    "\n",
    "\n",
    "for i in range(forTestLength):\n",
    "     if pred_result[i] == Y_real[forTestLength+i][0]:\n",
    "         count = count+1\n",
    "\n",
    "ck1=0\n",
    "ck2=0\n",
    "ck3=0\n",
    "for i in range(forTestLength):\n",
    "     if Y_real[forTestLength+i][0]== 1:\n",
    "         ck1 = ck1+1\n",
    "     elif Y_real[forTestLength + i][0] == 2:\n",
    "         ck2 = ck2 + 1\n",
    "     elif Y_real[forTestLength + i][0] == 3:\n",
    "         ck3 = ck3 + 1\n",
    "#scattetX = [ x for x in range(forTestLength)]\n",
    "print(\"ck1\", ck1)\n",
    "print(\"ck2\", ck2)\n",
    "print(\"ck3\", ck3)\n",
    "\n",
    "print(f1_score(Y[split+sequence_length:], pred_result[:], average='micro'))\n",
    "\n",
    "#ax = plt.subplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "confusion_matrix(pred_result[:],Y[split+sequence_length:])\n",
    "cm = confusion_matrix(pred_result[:],Y[split+sequence_length:], labels=[0,1,2])\n",
    "# cm=cm / cm.astype(np.float).sum(axis=1)\n",
    "# print(len(pred_result[:]))\n",
    "# print(len(Y[split+sequence_length:]))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1,2])\n",
    "disp.plot()\n",
    "\n",
    "plt.savefig('savefig_default.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5760bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scattetX = [ x for x in range(forTestLength)]\n",
    "#plt.figure(figsize=(20, 10))\n",
    "#plt.title(\"BMI prediction\")\n",
    "#plt.scatter(scattetX, Y[split+sequence_length:],  c='r', label='real value')          # 빨간색이 실제\n",
    "#plt.scatter(scattetX, pred_result[:], c='b', linewidth=0.6, label='prediction')             # 파란색이 예측\n",
    "\n",
    "#plt.legend()                                                          # 범례 적용\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochlist = [ x for x in range(len(loss_graph))]\n",
    "\n",
    "loss_graph = np.array(loss_graph)\n",
    "#loss_graph = np.reshape(1,-1)\n",
    "# print(loss_graph)\n",
    "# print(epochs)\n",
    "# print(len(loss_graph))\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochlist, loss_graph, label='train loss', color='red')\n",
    "plt.show()\n",
    "\n",
    "# print(\"count : \", count, \"length : \", forTestLength)\n",
    "# print(\"Accuracy: \", count/forTestLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_graph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69939c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochlist = [ x for x in range(len(accuracy_graph))]\n",
    "print(len(epochlist))\n",
    "accuracy_graph = np.array(accuracy_graph)\n",
    "accuracy_graph=accuracy_graph.reshape(500,-1)\n",
    "epochlist=np.array(epochlist)\n",
    "epochlist=epochlist.reshape(500,-1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(epochlist, accuracy_graph, label='train loss', color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(Y[split+sequence_length:], pred_result[:], average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Acc\",count/forTestLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f48d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
