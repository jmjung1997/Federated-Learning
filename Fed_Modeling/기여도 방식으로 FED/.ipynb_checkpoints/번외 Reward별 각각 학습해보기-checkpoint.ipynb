{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3795c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense,Dropout,Activation\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tensorflow.python.keras import metrics\n",
    "from tensorflow.python import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import math\n",
    "from category_encoders import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support as sk\n",
    "from sklearn.metrics import f1_score ## F1 Score 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd3f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230cb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../../Integrated_data/merged_df_reward.csv\",encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ed50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.interpolated()\n",
    "        self.abmormaled()\n",
    "        self.result=self.fill_missing_values()\n",
    "\n",
    "    # ... (다른 클래스 함수 정의는 여기에 있어야 합니다)\n",
    "\n",
    "    def interpolated(self):\n",
    "        temp=[]\n",
    "        temp2=[]\n",
    "        for i in df['ID'].unique():\n",
    "            try:\n",
    "                temp.append(df[df['ID']==i].interpolate())\n",
    "            except:\n",
    "                temp.append(df[df['ID']==i])\n",
    "\n",
    "        new_df=pd.concat(temp,axis=0)\n",
    "        new_df.fillna(method='bfill',inplace=True)   \n",
    "        for i in new_df['ID'].unique():\n",
    "            temp_df=new_df[new_df['ID']==i].reset_index(drop=True)\n",
    "            temp_df[\"height\"].fillna(method='bfill',inplace=True)\n",
    "            temp_df[\"weight\"].fillna(method='bfill',inplace=True)\n",
    "            temp_df[\"height\"].fillna(method='ffill',inplace=True)\n",
    "            temp_df[\"weight\"].fillna(method='ffill',inplace=True)\n",
    "            temp2.append(temp_df)\n",
    "        new_df=pd.concat(temp2,axis=0)  \n",
    "        self.df = new_df   \n",
    "    def gap(self,first, last):\n",
    "        gap_per=(last-first)/first*100\n",
    "        return abs(gap_per)\n",
    "        \n",
    "    def abmormaled(self):\n",
    "        temp = []\n",
    "        for i in self.df['ID'].unique():\n",
    "            temp_df = self.df[self.df['ID'] == i].reset_index(drop=True)\n",
    "            for i in range(1, len(temp_df)):\n",
    "                if temp_df.at[i, 'height'] <= temp_df.at[i - 1, 'height']:\n",
    "                    temp_df.at[i, 'height'] = temp_df.at[i - 1, 'height']\n",
    "                if self.gap(temp_df.at[i - 1, 'height'], temp_df.at[i, 'height']) >= 16:\n",
    "                    temp_df.at[i, 'height'] = temp_df.at[i - 1, 'height']\n",
    "            temp.append(temp_df)\n",
    "        new_df2 = pd.concat(temp, axis=0)\n",
    "        new_df2.reset_index(drop=True, inplace=True)\n",
    "        self.df = new_df2\n",
    "\n",
    "    def fill_missing_values(self):\n",
    "        self.df[\"step_count\"].fillna(self.df[\"step_count\"].median(), inplace=True)\n",
    "        self.df[\"burned calory\"].fillna(self.df[\"burned calory\"].median(), inplace=True)\n",
    "        self.df[\"eat_calory\"].fillna(self.df[\"eat_calory\"].median(), inplace=True)\n",
    "        self.df[\"Sleep_time\"].fillna(self.df[\"Sleep_time\"].median(), inplace=True)\n",
    "        return self.df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f335d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class TimeSeriesLabeler:\n",
    "    def __init__(self, term):\n",
    "        self.term = term\n",
    "        self.result_df = pd.DataFrame()\n",
    "        self.labels = []\n",
    "\n",
    "    def calculate_labels(self, df):\n",
    "        for id in df['ID'].unique():\n",
    "            df_id = df[df['ID'] == id].reset_index(drop=True)\n",
    "            label = []\n",
    "\n",
    "            for i in range(self.term, len(df_id)):\n",
    "                previous_days = df_id['weight'].iloc[i - self.term:i]\n",
    "                current_weight = df_id['weight'].iloc[i]\n",
    "                if current_weight > previous_days.mean().round(3):\n",
    "                    label.append(1)\n",
    "                elif current_weight == previous_days.mean().round(3):\n",
    "                    label.append(0)\n",
    "                elif current_weight < previous_days.mean().round(3):\n",
    "                    label.append(2)\n",
    "                else:\n",
    "                    print(\"error\")\n",
    "\n",
    "            self.labels.extend(label)\n",
    "\n",
    "    def create_result_df(self, df):\n",
    "        for id in df['ID'].unique():\n",
    "            df_id = df[df['ID'] == id].reset_index(drop=True)\n",
    "            df_id_drop = df_id.drop(index=range(self.term))\n",
    "            self.result_df = pd.concat([self.result_df, df_id_drop])\n",
    "        self.result_df['labels']= self.labels\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def get_result_df(self):\n",
    "        return self.result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e10032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    globals()['reward_data_{}'.format(i+1)]=pd.read_excel(\"../../Integrated_data/merged_df_reward_분리.xlsx\",sheet_name = i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9905eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reward_data_1\n",
      "Processing reward_data_2\n",
      "Processing reward_data_3\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임 리스트 생성\n",
    "dataframes = [reward_data_1, reward_data_2, reward_data_3]\n",
    "\n",
    "# 반복문으로 데이터프레임 순차적으로 호출\n",
    "for i, df in enumerate(dataframes, start=0):\n",
    "    print(f\"Processing reward_data_{i+1}\")\n",
    "    processor = DataProcessor(df)\n",
    "    globals()['df_{}'.format(i+1)]=processor.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d15eb",
   "metadata": {},
   "source": [
    "# ID 별로 일주일 간격으로 몸무게 변화 라벨링(유지:0, 감소:2, 증가:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1660b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 5  # You can set your desired 'term' here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f517e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler_1 = TimeSeriesLabeler(term)\n",
    "labeler_1.calculate_labels(df_1)  # Provide your DataFrame 'df' as input\n",
    "labeler_1.create_result_df(df_1)  # Create the result DataFrame\n",
    "labels = labeler_1.get_labels()\n",
    "result_df_1 = labeler_1.get_result_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ea4116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler_2 = TimeSeriesLabeler(term)\n",
    "labeler_2.calculate_labels(df_2)  # Provide your DataFrame 'df' as input\n",
    "labeler_2.create_result_df(df_2)  # Create the result DataFrame\n",
    "labels = labeler_2.get_labels()\n",
    "result_df_2 = labeler_2.get_result_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3784393",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler_3 = TimeSeriesLabeler(term)\n",
    "labeler_3.calculate_labels(df_3)  # Provide your DataFrame 'df' as input\n",
    "labeler_3.create_result_df(df_3)  # Create the result DataFrame\n",
    "labels = labeler_3.get_labels()\n",
    "result_df_3 = labeler_3.get_result_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce4adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>collect_datetime</th>\n",
       "      <th>gender</th>\n",
       "      <th>grade</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>step_count</th>\n",
       "      <th>burned calory</th>\n",
       "      <th>eat_calory</th>\n",
       "      <th>Sleep_time</th>\n",
       "      <th>서포터여부</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5490.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4919.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-23</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID collect_datetime  gender  grade  height  weight  step_count  \\\n",
       "5  #ASELLU       2022-07-19       2      6   164.0    64.0      5490.0   \n",
       "6  #ASELLU       2022-07-20       2      6   164.0    64.0      4100.0   \n",
       "7  #ASELLU       2022-07-21       2      6   164.0    64.0      4919.0   \n",
       "8  #ASELLU       2022-07-22       2      6   164.0    64.0       323.0   \n",
       "9  #ASELLU       2022-07-23       2      6   164.0    64.0       323.0   \n",
       "\n",
       "   burned calory  eat_calory  Sleep_time  서포터여부  labels  \n",
       "5          142.8       546.0       432.0      3       2  \n",
       "6          142.8       546.0       432.0      3       1  \n",
       "7          142.8       546.0       432.0      3       2  \n",
       "8          142.8       546.0       432.0      3       2  \n",
       "9          142.8       546.0       432.0      3       2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cadf7",
   "metadata": {},
   "source": [
    "# 데이터 세트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129c3360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>collect_datetime</th>\n",
       "      <th>gender</th>\n",
       "      <th>grade</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>step_count</th>\n",
       "      <th>burned calory</th>\n",
       "      <th>eat_calory</th>\n",
       "      <th>Sleep_time</th>\n",
       "      <th>서포터여부</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>45.15</td>\n",
       "      <td>72.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.10</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.10</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.10</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.10</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID collect_datetime  gender  grade  height  weight  step_count  \\\n",
       "5  #AAGQKY       2022-07-19       2      4   148.0   45.15        72.0   \n",
       "6  #AAGQKY       2022-07-20       2      4   148.0   28.10     10079.0   \n",
       "7  #AAGQKY       2022-07-21       2      4   148.0   28.10     10079.0   \n",
       "8  #AAGQKY       2022-07-22       2      4   148.0   28.10     10079.0   \n",
       "9  #AAGQKY       2022-07-23       2      4   148.0   28.10     10079.0   \n",
       "\n",
       "   burned calory  eat_calory  Sleep_time  서포터여부  labels  \n",
       "5         48.552      2000.0       468.0      2       0  \n",
       "6         48.552      2000.0       468.0      2       2  \n",
       "7         48.552      2000.0       468.0      2       2  \n",
       "8         48.552      2000.0       468.0      2       2  \n",
       "9         48.552      2000.0       468.0      2       2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601bbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=result_df_1.iloc[:,[4,5,6,7,8,9]]\n",
    "y_1=result_df_1.iloc[:,-1]\n",
    "\n",
    "X_2=result_df_2.iloc[:,[4,5,6,7,8,9]]\n",
    "y_2=result_df_2.iloc[:,-1]\n",
    "\n",
    "X_3=result_df_3.iloc[:,[4,5,6,7,8,9]]\n",
    "y_3=result_df_3.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2eac7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5      2\n",
       "6      2\n",
       "7      2\n",
       "8      0\n",
       "9      0\n",
       "      ..\n",
       "177    0\n",
       "178    0\n",
       "179    0\n",
       "180    0\n",
       "181    0\n",
       "Name: labels, Length: 37347, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2a575d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (37347, 6) (37347,)\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (100797, 6) (100797,)\n",
      "SMOTE 적용 전 레이블 값 분포: \n",
      " 0    33599\n",
      "1     2318\n",
      "2     1430\n",
      "Name: labels, dtype: int64\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      " 2    33599\n",
      "0    33599\n",
      "1    33599\n",
      "Name: labels, dtype: int64\n",
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (13983, 6) (13983,)\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (37818, 6) (37818,)\n",
      "SMOTE 적용 전 레이블 값 분포: \n",
      " 0    12606\n",
      "1      752\n",
      "2      625\n",
      "Name: labels, dtype: int64\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      " 0    12606\n",
      "2    12606\n",
      "1    12606\n",
      "Name: labels, dtype: int64\n",
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (4602, 6) (4602,)\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (10857, 6) (10857,)\n",
      "SMOTE 적용 전 레이블 값 분포: \n",
      " 0    3619\n",
      "1     534\n",
      "2     449\n",
      "Name: labels, dtype: int64\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      " 2    3619\n",
      "1    3619\n",
      "0    3619\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "scaler_1 = StandardScaler()\n",
    "scaler_2 = StandardScaler()\n",
    "scaler_3 = StandardScaler()\n",
    "\n",
    "X_scaler_1=scaler_1.fit_transform(X_1)\n",
    "X_scaler_2= scaler_2.fit_transform(X_2)\n",
    "X_scaler_3= scaler_3.fit_transform(X_3)\n",
    "\n",
    "\n",
    "smote_1 = SMOTE(random_state=0)\n",
    "X_over_1,y_over_1 = smote_1.fit_resample(X_scaler_1,y_1)\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_scaler_1.shape, y_1.shape)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_over_1.shape, y_over_1.shape)\n",
    "print('SMOTE 적용 전 레이블 값 분포: \\n', pd.Series(y_1).value_counts())\n",
    "print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_over_1).value_counts())\n",
    "\n",
    "smote_2 = SMOTE(random_state=0)\n",
    "X_over_2,y_over_2 = smote_2.fit_resample(X_scaler_2,y_2)\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_scaler_2.shape, y_2.shape)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_over_2.shape, y_over_2.shape)\n",
    "print('SMOTE 적용 전 레이블 값 분포: \\n', pd.Series(y_2).value_counts())\n",
    "print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_over_2).value_counts())\n",
    "\n",
    "smote_3 = SMOTE(random_state=0)\n",
    "X_over_3,y_over_3 = smote_3.fit_resample(X_scaler_3,y_3)\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_scaler_3.shape, y_3.shape)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_over_3.shape, y_over_3.shape)\n",
    "print('SMOTE 적용 전 레이블 값 분포: \\n', pd.Series(y_3).value_counts())\n",
    "print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_over_3).value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666df185",
   "metadata": {},
   "source": [
    "# 데이터 스케일링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c79e34",
   "metadata": {},
   "source": [
    "## Train, Test dataset 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbaf8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_over_1, y_over_1, test_size=0.20)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_over_2, y_over_2, test_size=0.20)\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_over_3, y_over_3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c6a0a",
   "metadata": {},
   "source": [
    "## Global train 데이터 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea2e90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_1,X_train_2,X_train_3), axis=0)\n",
    "y_train = np.concatenate((y_train_1,y_train_2,y_train_3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6abd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate((X_test_1,X_test_2,X_test_3), axis=0)\n",
    "y_test = np.concatenate((y_test_1,y_test_2,y_test_3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "203a40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "train_dataset=TensorDataset(X_train, y_train)\n",
    "test_dataset=TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a80589",
   "metadata": {},
   "source": [
    "## Client train 데이터 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "218c7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = torch.FloatTensor(X_train_1)\n",
    "X_test_1 = torch.FloatTensor(X_test_1)\n",
    "y_train_1 = torch.LongTensor(y_train_1.to_numpy())\n",
    "y_test_1 = torch.LongTensor(y_test_1.to_numpy())\n",
    "\n",
    "train_dataset_1=TensorDataset(X_train_1, y_train_1)\n",
    "test_dataset_1=TensorDataset(X_test_1, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e0bb6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = torch.FloatTensor(X_train_2)\n",
    "X_test_2 = torch.FloatTensor(X_test_2)\n",
    "y_train_2 = torch.LongTensor(y_train_2.to_numpy())\n",
    "y_test_2 = torch.LongTensor(y_test_2.to_numpy())\n",
    "\n",
    "train_dataset_2=TensorDataset(X_train_2, y_train_2)\n",
    "test_dataset_2=TensorDataset(X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c77bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = torch.FloatTensor(X_train_3)\n",
    "X_test_3 = torch.FloatTensor(X_test_3)\n",
    "y_train_3 = torch.LongTensor(y_train_3.to_numpy())\n",
    "y_test_3 = torch.LongTensor(y_test_3.to_numpy())\n",
    "\n",
    "train_dataset_3=TensorDataset(X_train_3, y_train_3)\n",
    "test_dataset_3=TensorDataset(X_test_3, y_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7c09eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf=StratifiedKFold(n_splits=10)\n",
    "# skf.get_n_splits(X_train_1,y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59a4d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(6, 128)\n",
    "        self.hidden_layer1 = nn.Linear(128, 256)\n",
    "        self.hidden_layer2 = nn.Linear(256, 128)\n",
    "        self.output_layer   = nn.Linear(128,3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  self.relu(self.input_layer(x))\n",
    "        out =  self.relu(self.hidden_layer1(out))\n",
    "        out =  self.relu(self.hidden_layer2(out))\n",
    "        out =  self.output_layer(out)\n",
    "        return out \n",
    "\n",
    "\n",
    "\n",
    "# device 설정 (cuda:0 혹은 cpu)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DNNModel() # Model 생성\n",
    "model.to(device)   # device 에 로드 (cpu or cuda)\n",
    "\n",
    "# 옵티마이저를 정의합니다. 옵티마이저에는 model.parameters()를 지정해야 합니다.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 손실함수(loss function)을 지정합니다. Multi-Class Classification 이기 때문에 CrossEntropy 손실을 지정하였습니다.\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4b169e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Progress Bar 출력\n",
    "\n",
    "def model_train(model, data_loader, loss_fn, optimizer, device):\n",
    "    # 모델을 훈련모드로 설정합니다. training mode 일 때 Gradient 가 업데이트 됩니다. 반드시 train()으로 모드 변경을 해야 합니다.\n",
    "    model.train()\n",
    "    # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "\n",
    "    # 예쁘게 Progress Bar를 출력하면서 훈련 상태를 모니터링 하기 위하여 tqdm으로 래핑합니다.\n",
    "    prograss_bar = tqdm(data_loader)\n",
    "\n",
    "    # mini-batch 학습을 시작합니다.\n",
    "    for data, lbl in prograss_bar:\n",
    "        # image, label 데이터를 device에 올립니다.\n",
    "        data, lbl = data.to(device), lbl.to(device)\n",
    "        # 누적 Gradient를 초기화 합니다.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Propagation을 진행하여 결과를 얻습니다.\n",
    "        output = model(data)\n",
    "\n",
    "        # 손실함수에 output, label 값을 대입하여 손실을 계산합니다.\n",
    "        loss = loss_fn(output, lbl)\n",
    "        # 오차역전파(Back Propagation)을 진행하여 미분 값을 계산합니다.\n",
    "        loss.backward()\n",
    "\n",
    "        # 계산된 Gradient를 업데이트 합니다.\n",
    "        optimizer.step()\n",
    "\n",
    "        # output의 max(dim=1)은 max probability와 max index를 반환합니다.\n",
    "        # max probability는 무시하고, max index는 pred에 저장하여 label 값과 대조하여 정확도를 도출합니다.\n",
    "        _, pred = output.max(dim=1)\n",
    "        # pred.eq(lbl).sum() 은 정확히 맞춘 label의 합계를 계산합니다. item()은 tensor에서 값을 추출합니다.\n",
    "        # 합계는 corr 변수에 누적합니다.\n",
    "        corr += pred.eq(lbl).sum().item()\n",
    "        \n",
    "        # loss 값은 1개 배치의 평균 손실(loss) 입니다. data.size(0)은 배치사이즈(batch size) 입니다.\n",
    "        # loss 와 data.size(0)를 곱하면 1개 배치의 전체 loss가 계산됩니다.\n",
    "        # 이를 누적한 뒤 Epoch 종료시 전체 데이터셋의 개수로 나누어 평균 loss를 산출합니다.\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "\n",
    "    # 누적된 정답수를 전체 개수로 나누어 주면 정확도가 산출됩니다.\n",
    "    acc = corr / len(data_loader.dataset)\n",
    "    # 평균 손실(loss)과 정확도를 반환합니다.\n",
    "    # train_loss, train_acc\n",
    "    return running_loss / len(data_loader.dataset), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f8bd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, data_loader, loss_fn, device):\n",
    "    # model.eval()은 모델을 평가모드로 설정을 바꾸어 줍니다. \n",
    "    # dropout과 같은 layer의 역할 변경을 위하여 evaluation 진행시 꼭 필요한 절차 입니다.\n",
    "    model.eval()\n",
    "    # Gradient가 업데이트 되는 것을 방지 하기 위하여 반드시 필요합니다.\n",
    "    with torch.no_grad():\n",
    "        # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # 배치별 evaluation을 진행합니다.\n",
    "        for data, lbl in data_loader:\n",
    "            # device에 데이터를 올립니다.\n",
    "            data, lbl = data.to(device), lbl.to(device)\n",
    "\n",
    "            # 모델에 Forward Propagation을 하여 결과를 도출합니다.\n",
    "            output = model(data)\n",
    "\n",
    "            # output의 max(dim=1)은 max probability와 max index를 반환합니다.\n",
    "            # max probability는 무시하고, max index는 pred에 저장하여 label 값과 대조하여 정확도를 도출합니다.\n",
    "            _, pred = output.max(dim=1)\n",
    "            \n",
    "\n",
    "            # pred.eq(lbl).sum() 은 정확히 맞춘 label의 합계를 계산합니다. item()은 tensor에서 값을 추출합니다.\n",
    "            # 합계는 corr 변수에 누적합니다.\n",
    "            corr += torch.sum(pred.eq(lbl)).item()\n",
    "            \n",
    "            # loss 값은 1개 배치의 평균 손실(loss) 입니다. data.size(0)은 배치사이즈(batch size) 입니다.\n",
    "            # loss 와 data.size(0)를 곱하면 1개 배치의 전체 loss가 계산됩니다.\n",
    "            # 이를 누적한 뒤 Epoch 종료시 전체 데이터셋의 개수로 나누어 평균 loss를 산출합니다.\n",
    "            running_loss += loss_fn(output, lbl).item() * data.size(0)\n",
    "\n",
    "        # validation 정확도를 계산합니다.\n",
    "        # 누적한 정답숫자를 전체 데이터셋의 숫자로 나누어 최종 accuracy를 산출합니다.\n",
    "        acc = corr / len(data_loader.dataset)\n",
    "\n",
    "        # 결과를 반환합니다.\n",
    "        # val_loss, val_acc\n",
    "        return running_loss / len(data_loader.dataset), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4de01d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, data_loader, loss_fn, device):\n",
    "    # model.eval()은 모델을 평가모드로 설정을 바꾸어 줍니다. \n",
    "    # dropout과 같은 layer의 역할 변경을 위하여 evaluation 진행시 꼭 필요한 절차 입니다.\n",
    "    model.eval()\n",
    "    pred_list=[]\n",
    "    # Gradient가 업데이트 되는 것을 방지 하기 위하여 반드시 필요합니다.\n",
    "    with torch.no_grad():\n",
    "        # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # 배치별 evaluation을 진행합니다.\n",
    "        for data, lbl in data_loader:\n",
    "            # device에 데이터를 올립니다.\n",
    "            data, lbl = data.to(device), lbl.to(device)\n",
    "\n",
    "            # 모델에 Forward Propagation을 하여 결과를 도출합니다.\n",
    "            output = model(data)\n",
    "            \n",
    "            \n",
    "            # output의 max(dim=1)은 max probability와 max index를 반환합니다.\n",
    "            # max probability는 무시하고, max index는 pred에 저장하여 label 값과 대조하여 정확도를 도출합니다.\n",
    "            _, pred = output.max(dim=1)\n",
    "            pred_array = pred.tolist()\n",
    "            pred_list.append(pred_array) # confusion matrix를 위해 pred 리턴 값\n",
    "            # pred.eq(lbl).sum() 은 정확히 맞춘 label의 합계를 계산합니다. item()은 tensor에서 값을 추출합니다.\n",
    "            # 합계는 corr 변수에 누적합니다.\n",
    "            corr += torch.sum(pred.eq(lbl)).item()\n",
    "            \n",
    "            # loss 값은 1개 배치의 평균 손실(loss) 입니다. data.size(0)은 배치사이즈(batch size) 입니다.\n",
    "            # loss 와 data.size(0)를 곱하면 1개 배치의 전체 loss가 계산됩니다.\n",
    "            # 이를 누적한 뒤 Epoch 종료시 전체 데이터셋의 개수로 나누어 평균 loss를 산출합니다.\n",
    "            running_loss += loss_fn(output, lbl).item() * data.size(0)\n",
    "\n",
    "        # validation 정확도를 계산합니다.\n",
    "        # 누적한 정답숫자를 전체 데이터셋의 숫자로 나누어 최종 accuracy를 산출합니다.\n",
    "        acc = corr / len(data_loader.dataset)\n",
    "\n",
    "        # 결과를 반환합니다.\n",
    "        # val_loss, val_acc\n",
    "        return running_loss / len(data_loader.dataset), acc, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "199c1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_over=X_over_1\n",
    "y_train_over=y_over_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:01<00:00, 61.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_acc has been improved from 9999999999999.00000 to 1.15816. Saving Model!\n",
      "epoch 01, loss: 1.65814, acc: 0.49772, val_loss: 1.15816, val_accuracy: 0.33333\n",
      "0번째 교차검증\n",
      "evaluation loss: 1.15816, evaluation accuracy: 0.33333\n"
     ]
    }
   ],
   "source": [
    "# X_test=pd.DataFrame()\n",
    "# X_train=pd.DataFrame()\n",
    "# y_test=pd.DataFrame()\n",
    "# y_train=pd.DataFrame()\n",
    "# empty=pd.DataFrame()\n",
    "\n",
    "#결과 넣을 배열\n",
    "Result=[[0 for j in range(4)] for i in range(10)]\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf=StratifiedKFold(n_splits=10)\n",
    "\n",
    "#결과 넣을 배열\n",
    "Result=[[0 for j in range(4)] for i in range(10)]\n",
    "CMResult=[[[0 for k in range(3)]for j in range(3)] for i in range(10)]\n",
    "skf.get_n_splits(X_train_over,y_train_over)\n",
    "\n",
    "i=0\n",
    "for tfold, (train_index, test_index) in enumerate(skf.split(X_train_over, y_train_over)):\n",
    "    #모델 정의, 예측\n",
    "\n",
    "    X_train, X_test = X_train_over[train_index], X_train_over[test_index]\n",
    "    y_train, y_test = y_train_over[train_index], y_train_over[test_index]\n",
    "\n",
    "    #DNN\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_train = torch.LongTensor(y_train.to_numpy())\n",
    "    y_test = torch.LongTensor(y_test.to_numpy())\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset=TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1000,shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=500,shuffle=False)\n",
    "\n",
    "    # 최대 Epoch을 지정합니다.\n",
    "    num_epochs = 1\n",
    "    min_loss = 9999999999999\n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    # Epoch 별 훈련 및 검증을 수행합니다.\n",
    "    for epoch in range(num_epochs):\n",
    "        # Model Training\n",
    "        # 훈련 손실과 정확도를 반환 받습니다.\n",
    "        train_loss, train_acc = model_train(model, train_dataloader, loss_fn, optimizer, device)\n",
    "\n",
    "        # 검증 손실과 검증 정확도를 반환 받습니다.\n",
    "        val_loss, val_acc = model_evaluate(model, test_dataloader, loss_fn, device)   \n",
    "\n",
    "        # val_loss 가 개선되었다면 min_loss를 갱신하고 model의 가중치(weights)를 저장합니다.\n",
    "        if val_loss < min_loss:\n",
    "                print(f'[INFO] val_acc has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "                min_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'DNNModel.pth')\n",
    "\n",
    "        # Epoch 별 결과를 출력합니다.\n",
    "        print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_acc:.5f}')\n",
    "        loss_list.append(train_loss)\n",
    "        acc_list.append(train_acc)\n",
    "    ## 저장한 가중치 로드 후 검증 성능 측정\n",
    "\n",
    "    # 모델에 저장한 가중치를 로드합니다.\n",
    "    model.load_state_dict(torch.load('DNNModel.pth'))\n",
    "\n",
    "    # 최종 검증 손실(validation loss)와 검증 정확도(validation accuracy)를 산출합니다.\n",
    "    print(\"{}번째 교차검증\".format(tfold))\n",
    "    final_loss, final_acc, y_predicted = model_test(model, test_dataloader, loss_fn, device)\n",
    "    print(f'evaluation loss: {final_loss:.5f}, evaluation accuracy: {final_acc:.5f}')   \n",
    "\n",
    "    X_train, X_test = X_train_over[train_index], X_train_over[test_index]\n",
    "    y_train, y_test = y_train_over[train_index], y_train_over[test_index]\n",
    "\n",
    "#     ## 원핫 인코딩\n",
    "#     y_train=pd.get_dummies(y_train)\n",
    "#     y_test=pd.get_dummies(y_test)\n",
    "\n",
    "\n",
    "    # LinearDiscriminantAnalysis\n",
    "    ld = LinearDiscriminantAnalysis()\n",
    "    ld.fit(X_train, y_train)\n",
    "    linear_pred = ld.predict(X_test)\n",
    "    # linear_pred = lr.predict(X_train)\n",
    "    \n",
    "    # DecisionTreeClassifier\n",
    "    dt = DecisionTreeClassifier(max_depth=6)\n",
    "    dt.fit(X_train, y_train)\n",
    "    decisione_pred = dt.predict(X_test)\n",
    "    # ridge_pred = clf.predict(X_train)\n",
    "    \n",
    "    # RandomForestClassifier\n",
    "    regr = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "    regr.fit(X_train, y_train)\n",
    "    regr_pred = regr.predict(X_test)\n",
    "    # regr_pred = regr.predict(X_train)\n",
    "    \n",
    "    # GaussianNB\n",
    "    gs = GaussianNB()\n",
    "    gs.fit(X_train, y_train)\n",
    "    gs_pred = gs.predict(X_test)\n",
    "    # reg_pred = reg.predict(X_train)\n",
    "    \n",
    "    #SVM\n",
    "    SVM = SVC(kernel='rbf', C=8, gamma=0.1)\n",
    "    SVM.fit(X_train,y_train) # rbf Kernel\n",
    "    SVM_pred = SVM.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # DNN 결과값 넘파이 배열로 변환\n",
    "    y_predicted_t=sum(y_predicted, [])\n",
    "    DNN_y_predicted=np.array(y_predicted_t)\n",
    "\n",
    "    # 최종모델\n",
    "    \n",
    "    lgb_param={'max_depth':10,'learning_rate':0.01,'n_estimators':100,'objective':'multiclass','num_class':len(set(y_train))+1}\n",
    "    lgbm = LGBMClassifier(**lgb_param)\n",
    "\n",
    "    new_data = np.array([linear_pred.squeeze(),decisione_pred.squeeze(),SVM_pred.squeeze(),regr_pred.squeeze(),gs_pred.squeeze(),DNN_y_predicted.squeeze()])\n",
    "    # new_data = np.array([linear_pred,ridge_pred,regr_pred,reg_pred,DNN_y_predicted])\n",
    "    print(new_data.shape)\n",
    "\n",
    "    new_data = np.transpose(new_data)\n",
    "    print(new_data.shape)\n",
    "\n",
    "    #y_test를 파이토치에서 넘파이 배열로 변환\n",
    "    y_test=np.array(y_test)\n",
    "    \n",
    "    #lgbm 학습하기\n",
    "    lgbm.fit(new_data,y_test)\n",
    "    # lgbm.fit(new_data, y_train)\n",
    "    \n",
    "    #LGBMClassifier 예측하기\n",
    "    lgbm_pred = lgbm.predict(new_data)\n",
    "    # lgbm_pred = lgbm.predict(X_test)\n",
    "\n",
    "    print('Stacking ensemble (ML+DNN):',len(lgbm_pred))\n",
    "    \n",
    "    \n",
    "    accuracy=accuracy_score(y_test, lgbm_pred) \n",
    "    print(\"[{}]Accuracy : {}\".format(tfold,accuracy))   \n",
    "    #f1score\n",
    "    f1 = f1_score(y_test,lgbm_pred, average='weighted')\n",
    "    print(\"[{}]F1score : {}\".format(tfold,f1))\n",
    "    #precision/recall\n",
    "    p_rlist=sk(y_test,lgbm_pred,average='weighted')\n",
    "    print(\"[{}]Precision : {}\".format(tfold,p_rlist[0]))\n",
    "    print(\"[{}]Recall : {}\".format(tfold,p_rlist[1]))\n",
    "    print()\n",
    "     #결과 배열에 넣기\n",
    "    Result[tfold][0]=accuracy\n",
    "    Result[tfold][1]=f1\n",
    "    Result[tfold][2]=p_rlist[0]\n",
    "    Result[tfold][3]=p_rlist[1]\n",
    "    del accuracy\n",
    "    del f1\n",
    "    del p_rlist\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    confusion_matrix(y_test, lgbm_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_test, lgbm_pred, labels=[0,1,2],normalize=\"true\")\n",
    "    CMResult[i]=cm\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[0,1,2])\n",
    "    disp.plot()\n",
    "    filename=(\"ConfusionMatrix_\"+\"가우,랜포,LDA,SVM,grid\"+str(i))\n",
    "    plt.savefig(\"./ConfusionMatrix_reward1/\"+filename+\".png\")\n",
    "    plt.show()\n",
    "    \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_df=pd.DataFrame(Result,columns=['Accuracy','F1-Score','Precision','Recall'])\n",
    "Result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354bae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix=pd.DataFrame(Result_df['Accuracy'],columns=['Accuracy'])\n",
    "Matrix['Accuracy']=Result_df['Accuracy']\n",
    "A=[Result_df['Accuracy'].mean(),Result_df['F1-Score'].mean(),Result_df['Precision'].mean(),Result_df['Recall'].mean()]\n",
    "A=pd.DataFrame(A,columns=['Accuracy'])\n",
    "Matrix=pd.concat([Matrix,A])\n",
    "Matrix=Matrix.transpose()\n",
    "Matrix.to_excel('./PFMatrix_ensemble2_final2_grid_reward1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672df0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad10a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667beb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386001dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9063071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cc89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7dbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d32f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3e585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0402def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c55ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab3371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542dd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e95669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aacf4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1616c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "865eecc6",
   "metadata": {},
   "source": [
    "# 파라미터 값 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size=len(train_dataset)\n",
    "\n",
    "\n",
    "total_train_size_1 = len(train_dataset_1)\n",
    "total_test_size_1 = len(test_dataset_1)\n",
    "\n",
    "total_train_size_2 = len(train_dataset_2)\n",
    "total_test_size_2 = len(test_dataset_2)\n",
    "\n",
    "total_train_size_3 = len(train_dataset_3)\n",
    "total_test_size_3 = len(test_dataset_3)\n",
    "\n",
    "classes = 3\n",
    "input_dim = 8\n",
    "\n",
    "num_clients = 3\n",
    "rounds = 10\n",
    "batch_size = 7000\n",
    "epochs_per_client = 10\n",
    "learning_rate = 2e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfb4bb",
   "metadata": {},
   "source": [
    "# GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b40f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345dbe42",
   "metadata": {},
   "source": [
    "# 딥러닝 모델 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1400d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 128)\n",
    "        self.hidden_layer1 = nn.Linear(128, 256)\n",
    "        self.hidden_layer2 = nn.Linear(256, 128)\n",
    "        self.output_layer   = nn.Linear(128,3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.track_layers = {'hidden_layer1': self.hidden_layer1, 'hidden_layer2': self.hidden_layer2, 'output_layer': self.output_layer}\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  self.relu(self.input_layer(x))\n",
    "        out =  self.relu(self.hidden_layer1(out))\n",
    "        out =  self.relu(self.hidden_layer2(out))\n",
    "        out =  self.output_layer(out)\n",
    "        return out \n",
    "    \n",
    "    def get_track_layers(self):\n",
    "        return self.track_layers\n",
    "    \n",
    "    def apply_parameters(self, parameters_dict):\n",
    "        with torch.no_grad():\n",
    "            for layer_name in parameters_dict:\n",
    "                self.track_layers[layer_name].weight.data *= 0\n",
    "                self.track_layers[layer_name].bias.data *= 0\n",
    "                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n",
    "                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        parameters_dict = dict()\n",
    "        for layer_name in self.track_layers:\n",
    "            parameters_dict[layer_name] = {\n",
    "                'weight': self.track_layers[layer_name].weight.data, \n",
    "                'bias': self.track_layers[layer_name].bias.data\n",
    "            }\n",
    "        return parameters_dict\n",
    "    \n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        \n",
    "        data, labels = batch\n",
    "        outputs= self(data)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels.squeeze(dim=-1))\n",
    "        accuracy = self.batch_accuracy(outputs, labels.squeeze(dim=-1))\n",
    "        return (loss, accuracy)\n",
    "    \n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        print(\"evaluate_losses.,shape\",np.array(losses).shape)\n",
    "        print(\"evaluate_avg_loss\",avg_loss)\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f73021",
   "metadata": {},
   "source": [
    "# 클라이언트 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "    \n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(DNNModel(), device)\n",
    "        net.apply_parameters(parameters_dict)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n",
    "        return net.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b2cdd",
   "metadata": {},
   "source": [
    "# 클라이언트 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_datasets=[train_dataset_1,train_dataset_2,train_dataset_3]\n",
    "\n",
    "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408775e0",
   "metadata": {},
   "source": [
    "# 중앙 서버 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ed4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(DNNModel(), device)\n",
    "history = []\n",
    "j=0\n",
    "for i in range(rounds):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = global_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for index, client in enumerate(clients):\n",
    "    \n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "    global_net.apply_parameters(new_parameters)\n",
    "    \n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, dev_loss = {}, ,train_acc={}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n",
    "            round(dev_loss, 4),round(train_acc, 4) ,round(dev_acc, 4)))\n",
    "    history.append((train_loss, dev_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d28d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd0f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0113f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ceb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324abe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5286aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17845f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
