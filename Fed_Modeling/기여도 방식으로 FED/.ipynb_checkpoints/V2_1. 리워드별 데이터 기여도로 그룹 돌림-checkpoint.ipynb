{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd3f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230cb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../../Integrated_data/merged_df_reward.csv\",encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ed50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.interpolated()\n",
    "        self.abmormaled()\n",
    "        self.result=self.fill_missing_values()\n",
    "\n",
    "    # ... (다른 클래스 함수 정의는 여기에 있어야 합니다)\n",
    "\n",
    "    def interpolated(self):\n",
    "        temp=[]\n",
    "        temp2=[]\n",
    "        for i in df['ID'].unique():\n",
    "            try:\n",
    "                temp.append(df[df['ID']==i].interpolate())\n",
    "            except:\n",
    "                temp.append(df[df['ID']==i])\n",
    "\n",
    "        new_df=pd.concat(temp,axis=0)\n",
    "        new_df.fillna(method='bfill',inplace=True)   \n",
    "        for i in new_df['ID'].unique():\n",
    "            temp_df=new_df[new_df['ID']==i].reset_index(drop=True)\n",
    "            temp_df[\"height\"].fillna(method='bfill',inplace=True)\n",
    "            temp_df[\"weight\"].fillna(method='bfill',inplace=True)\n",
    "            temp_df[\"height\"].fillna(method='ffill',inplace=True)\n",
    "            temp_df[\"weight\"].fillna(method='ffill',inplace=True)\n",
    "            temp2.append(temp_df)\n",
    "        new_df=pd.concat(temp2,axis=0)  \n",
    "        self.df = new_df   \n",
    "    def gap(self,first, last):\n",
    "        gap_per=(last-first)/first*100\n",
    "        return abs(gap_per)\n",
    "        \n",
    "    def abmormaled(self):\n",
    "        temp = []\n",
    "        for i in self.df['ID'].unique():\n",
    "            temp_df = self.df[self.df['ID'] == i].reset_index(drop=True)\n",
    "            for i in range(1, len(temp_df)):\n",
    "                if temp_df.at[i, 'height'] <= temp_df.at[i - 1, 'height']:\n",
    "                    temp_df.at[i, 'height'] = temp_df.at[i - 1, 'height']\n",
    "                if self.gap(temp_df.at[i - 1, 'height'], temp_df.at[i, 'height']) >= 16:\n",
    "                    temp_df.at[i, 'height'] = temp_df.at[i - 1, 'height']\n",
    "            temp.append(temp_df)\n",
    "        new_df2 = pd.concat(temp, axis=0)\n",
    "        new_df2.reset_index(drop=True, inplace=True)\n",
    "        self.df = new_df2\n",
    "\n",
    "    def fill_missing_values(self):\n",
    "        self.df[\"step_count\"].fillna(self.df[\"step_count\"].median(), inplace=True)\n",
    "        self.df[\"burned calory\"].fillna(self.df[\"burned calory\"].median(), inplace=True)\n",
    "        self.df[\"eat_calory\"].fillna(self.df[\"eat_calory\"].median(), inplace=True)\n",
    "        self.df[\"Sleep_time\"].fillna(self.df[\"Sleep_time\"].median(), inplace=True)\n",
    "        return self.df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f335d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class TimeSeriesLabeler:\n",
    "    def __init__(self, term):\n",
    "        self.term = term\n",
    "        self.result_df = pd.DataFrame()\n",
    "        self.labels = []\n",
    "\n",
    "    def calculate_labels(self, df):\n",
    "        for id in df['ID'].unique():\n",
    "            df_id = df[df['ID'] == id].reset_index(drop=True)\n",
    "            label = []\n",
    "\n",
    "            for i in range(self.term, len(df_id)):\n",
    "                previous_days = df_id['weight'].iloc[i - self.term:i]\n",
    "                current_weight = df_id['weight'].iloc[i]\n",
    "                if current_weight > previous_days.mean().round(3):\n",
    "                    label.append(1)\n",
    "                elif current_weight == previous_days.mean().round(3):\n",
    "                    label.append(0)\n",
    "                elif current_weight < previous_days.mean().round(3):\n",
    "                    label.append(2)\n",
    "                else:\n",
    "                    print(\"error\")\n",
    "\n",
    "            self.labels.extend(label)\n",
    "\n",
    "    def create_result_df(self, df):\n",
    "        for id in df['ID'].unique():\n",
    "            df_id = df[df['ID'] == id].reset_index(drop=True)\n",
    "            df_id_drop = df_id.drop(index=range(self.term))\n",
    "            self.result_df = pd.concat([self.result_df, df_id_drop])\n",
    "        self.result_df['labels']= self.labels\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def get_result_df(self):\n",
    "        return self.result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e10032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    globals()['reward_data_{}'.format(i+1)]=pd.read_excel(\"../../Integrated_data/merged_df_reward_분리.xlsx\",sheet_name = i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9905eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reward_data_1\n",
      "Processing reward_data_2\n",
      "Processing reward_data_3\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임 리스트 생성\n",
    "dataframes = [reward_data_1, reward_data_2, reward_data_3]\n",
    "\n",
    "# 반복문으로 데이터프레임 순차적으로 호출\n",
    "for i, df in enumerate(dataframes, start=0):\n",
    "    print(f\"Processing reward_data_{i+1}\")\n",
    "    processor = DataProcessor(df)\n",
    "    globals()['df_{}'.format(i+1)]=processor.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d15eb",
   "metadata": {},
   "source": [
    "# ID 별로 일주일 간격으로 몸무게 변화 라벨링(유지:0, 감소:2, 증가:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1660b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 7  # You can set your desired 'term' here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f517e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler_1 = TimeSeriesLabeler(term)\n",
    "labeler_1.calculate_labels(df_1)  # Provide your DataFrame 'df' as input\n",
    "labeler_1.create_result_df(df_1)  # Create the result DataFrame\n",
    "labels = labeler_1.get_labels()\n",
    "result_df_1 = labeler_1.get_result_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea4116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler_2 = TimeSeriesLabeler(term)\n",
    "labeler_2.calculate_labels(df_2)  # Provide your DataFrame 'df' as input\n",
    "labeler_2.create_result_df(df_2)  # Create the result DataFrame\n",
    "labels = labeler_2.get_labels()\n",
    "result_df_2 = labeler_2.get_result_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3784393",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler_3 = TimeSeriesLabeler(term)\n",
    "labeler_3.calculate_labels(df_3)  # Provide your DataFrame 'df' as input\n",
    "labeler_3.create_result_df(df_3)  # Create the result DataFrame\n",
    "labels = labeler_3.get_labels()\n",
    "result_df_3 = labeler_3.get_result_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce4adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>collect_datetime</th>\n",
       "      <th>gender</th>\n",
       "      <th>grade</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>step_count</th>\n",
       "      <th>burned calory</th>\n",
       "      <th>eat_calory</th>\n",
       "      <th>Sleep_time</th>\n",
       "      <th>서포터여부</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4919.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-23</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-24</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#ASELLU</td>\n",
       "      <td>2022-07-25</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>164.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>142.8</td>\n",
       "      <td>546.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID collect_datetime  gender  grade  height  weight  step_count  \\\n",
       "7   #ASELLU       2022-07-21       2      6   164.0    64.0      4919.0   \n",
       "8   #ASELLU       2022-07-22       2      6   164.0    64.0       323.0   \n",
       "9   #ASELLU       2022-07-23       2      6   164.0    64.0       323.0   \n",
       "10  #ASELLU       2022-07-24       2      6   164.0    64.0      1898.0   \n",
       "11  #ASELLU       2022-07-25       2      6   164.0    64.0      1072.0   \n",
       "\n",
       "    burned calory  eat_calory  Sleep_time  서포터여부  labels  \n",
       "7           142.8       546.0       432.0      3       2  \n",
       "8           142.8       546.0       432.0      3       1  \n",
       "9           142.8       546.0       432.0      3       2  \n",
       "10          142.8       546.0       432.0      3       2  \n",
       "11          142.8       546.0       432.0      3       2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cadf7",
   "metadata": {},
   "source": [
    "# 데이터 세트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129c3360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>collect_datetime</th>\n",
       "      <th>gender</th>\n",
       "      <th>grade</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>step_count</th>\n",
       "      <th>burned calory</th>\n",
       "      <th>eat_calory</th>\n",
       "      <th>Sleep_time</th>\n",
       "      <th>서포터여부</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-24</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-25</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>148.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>10079.0</td>\n",
       "      <td>48.552</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID collect_datetime  gender  grade  height  weight  step_count  \\\n",
       "7   #AAGQKY       2022-07-21       2      4   148.0    28.1     10079.0   \n",
       "8   #AAGQKY       2022-07-22       2      4   148.0    28.1     10079.0   \n",
       "9   #AAGQKY       2022-07-23       2      4   148.0    28.1     10079.0   \n",
       "10  #AAGQKY       2022-07-24       2      4   148.0    28.1     10079.0   \n",
       "11  #AAGQKY       2022-07-25       2      4   148.0    28.1     10079.0   \n",
       "\n",
       "    burned calory  eat_calory  Sleep_time  서포터여부  labels  \n",
       "7          48.552      2000.0       468.0      2       2  \n",
       "8          48.552      2000.0       468.0      2       2  \n",
       "9          48.552      2000.0       468.0      2       2  \n",
       "10         48.552      2000.0       468.0      2       2  \n",
       "11         48.552      2000.0       468.0      2       2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "601bbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=result_df_1.iloc[:,[2,3,4,5,6,7,8,9]]\n",
    "y_1=result_df_1.iloc[:,-1].to_frame()\n",
    "\n",
    "X_2=result_df_2.iloc[:,[2,3,4,5,6,7,8,9]]\n",
    "y_2=result_df_2.iloc[:,-1].to_frame()\n",
    "\n",
    "X_3=result_df_3.iloc[:,[2,3,4,5,6,7,8,9]]\n",
    "y_3=result_df_3.iloc[:,-1].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2a575d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "scaler_1 = StandardScaler()\n",
    "scaler_2 = StandardScaler()\n",
    "scaler_3 = StandardScaler()\n",
    "\n",
    "X_scaler_1=scaler_1.fit_transform(X_1)\n",
    "X_scaler_2= scaler_2.fit_transform(X_2)\n",
    "X_scaler_3= scaler_3.fit_transform(X_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666df185",
   "metadata": {},
   "source": [
    "# 데이터 스케일링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c79e34",
   "metadata": {},
   "source": [
    "## Train, Test dataset 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbaf8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_scaler_1, y_1, test_size=0.20)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_scaler_2, y_2, test_size=0.20)\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_scaler_3, y_3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c6a0a",
   "metadata": {},
   "source": [
    "## Global train 데이터 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2e90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_1,X_train_2,X_train_3), axis=0)\n",
    "y_train = np.concatenate((y_train_1,y_train_2,y_train_3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6abd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate((X_test_1,X_test_2,X_test_3), axis=0)\n",
    "y_test = np.concatenate((y_test_1,y_test_2,y_test_3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203a40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "train_dataset=TensorDataset(X_train, y_train)\n",
    "test_dataset=TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a80589",
   "metadata": {},
   "source": [
    "## Client train 데이터 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218c7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = torch.FloatTensor(X_train_1)\n",
    "X_test_1 = torch.FloatTensor(X_test_1)\n",
    "y_train_1 = torch.LongTensor(y_train_1.to_numpy())\n",
    "y_test_1 = torch.LongTensor(y_test_1.to_numpy())\n",
    "\n",
    "train_dataset_1=TensorDataset(X_train_1, y_train_1)\n",
    "test_dataset_1=TensorDataset(X_test_1, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e0bb6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = torch.FloatTensor(X_train_2)\n",
    "X_test_2 = torch.FloatTensor(X_test_2)\n",
    "y_train_2 = torch.LongTensor(y_train_2.to_numpy())\n",
    "y_test_2 = torch.LongTensor(y_test_2.to_numpy())\n",
    "\n",
    "train_dataset_2=TensorDataset(X_train_2, y_train_2)\n",
    "test_dataset_2=TensorDataset(X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c77bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = torch.FloatTensor(X_train_3)\n",
    "X_test_3 = torch.FloatTensor(X_test_3)\n",
    "y_train_3 = torch.LongTensor(y_train_3.to_numpy())\n",
    "y_test_3 = torch.LongTensor(y_test_3.to_numpy())\n",
    "\n",
    "train_dataset_3=TensorDataset(X_train_3, y_train_3)\n",
    "test_dataset_3=TensorDataset(X_test_3, y_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865eecc6",
   "metadata": {},
   "source": [
    "# 파라미터 값 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "171b63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size=len(train_dataset)\n",
    "\n",
    "\n",
    "total_train_size_1 = len(train_dataset_1)\n",
    "total_test_size_1 = len(test_dataset_1)\n",
    "\n",
    "total_train_size_2 = len(train_dataset_2)\n",
    "total_test_size_2 = len(test_dataset_2)\n",
    "\n",
    "total_train_size_3 = len(train_dataset_3)\n",
    "total_test_size_3 = len(test_dataset_3)\n",
    "\n",
    "classes = 3\n",
    "input_dim = 8\n",
    "\n",
    "num_clients = 3\n",
    "rounds = 10\n",
    "batch_size = 7000\n",
    "epochs_per_client = 10\n",
    "learning_rate = 2e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfb4bb",
   "metadata": {},
   "source": [
    "# GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b40f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345dbe42",
   "metadata": {},
   "source": [
    "# 딥러닝 모델 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1400d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 128)\n",
    "        self.hidden_layer1 = nn.Linear(128, 256)\n",
    "        self.hidden_layer2 = nn.Linear(256, 128)\n",
    "        self.output_layer   = nn.Linear(128,3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.track_layers = {'hidden_layer1': self.hidden_layer1, 'hidden_layer2': self.hidden_layer2, 'output_layer': self.output_layer}\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  self.relu(self.input_layer(x))\n",
    "        out =  self.relu(self.hidden_layer1(out))\n",
    "        out =  self.relu(self.hidden_layer2(out))\n",
    "        out =  self.output_layer(out)\n",
    "        return out \n",
    "    \n",
    "    def get_track_layers(self):\n",
    "        return self.track_layers\n",
    "    \n",
    "    def apply_parameters(self, parameters_dict):\n",
    "        with torch.no_grad():\n",
    "            for layer_name in parameters_dict:\n",
    "                self.track_layers[layer_name].weight.data *= 0\n",
    "                self.track_layers[layer_name].bias.data *= 0\n",
    "                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n",
    "                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        parameters_dict = dict()\n",
    "        for layer_name in self.track_layers:\n",
    "            parameters_dict[layer_name] = {\n",
    "                'weight': self.track_layers[layer_name].weight.data, \n",
    "                'bias': self.track_layers[layer_name].bias.data\n",
    "            }\n",
    "        return parameters_dict\n",
    "    \n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        \n",
    "        data, labels = batch\n",
    "        outputs= self(data)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels.squeeze(dim=-1))\n",
    "        accuracy = self.batch_accuracy(outputs, labels.squeeze(dim=-1))\n",
    "        return (loss, accuracy)\n",
    "    \n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        print(\"evaluate_losses.,shape\",np.array(losses).shape)\n",
    "        print(\"evaluate_avg_loss\",avg_loss)\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f73021",
   "metadata": {},
   "source": [
    "# 클라이언트 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cafb6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "    \n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(DNNModel(), device)\n",
    "        net.apply_parameters(parameters_dict)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n",
    "        return net.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b2cdd",
   "metadata": {},
   "source": [
    "# 클라이언트 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "492c17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_datasets=[train_dataset_1,train_dataset_2,train_dataset_3]\n",
    "\n",
    "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408775e0",
   "metadata": {},
   "source": [
    "# 중앙 서버 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "165ed4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Round 1 ...\n",
      "client_0: Loss = 0.5418, Accuracy = 0.8757\n",
      "client_1: Loss = 0.8206, Accuracy = 0.8816\n",
      "client_2: Loss = 1.0353, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.6463094353675842\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.6506230235099792\n",
      "After round 1, train_loss = 0.6463, dev_loss = 0.6506, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 2 ...\n",
      "client_0: Loss = 0.4764, Accuracy = 0.8768\n",
      "client_1: Loss = 0.5059, Accuracy = 0.881\n",
      "client_2: Loss = 0.8126, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5202930569648743\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.5278372764587402\n",
      "After round 2, train_loss = 0.5203, dev_loss = 0.5278, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 3 ...\n",
      "client_0: Loss = 0.4658, Accuracy = 0.8781\n",
      "client_1: Loss = 0.4705, Accuracy = 0.8821\n",
      "client_2: Loss = 0.8158, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5069785714149475\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.5157949924468994\n",
      "After round 3, train_loss = 0.507, dev_loss = 0.5158, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 4 ...\n",
      "client_0: Loss = 0.4729, Accuracy = 0.8751\n",
      "client_1: Loss = 0.4609, Accuracy = 0.8815\n",
      "client_2: Loss = 0.8117, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5059828162193298\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.515331506729126\n",
      "After round 4, train_loss = 0.506, dev_loss = 0.5153, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 5 ...\n",
      "client_0: Loss = 0.4686, Accuracy = 0.8753\n",
      "client_1: Loss = 0.4561, Accuracy = 0.8806\n",
      "client_2: Loss = 0.825, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5053139328956604\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.5146496295928955\n",
      "After round 5, train_loss = 0.5053, dev_loss = 0.5146, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 6 ...\n",
      "client_0: Loss = 0.4626, Accuracy = 0.8773\n",
      "client_1: Loss = 0.464, Accuracy = 0.8816\n",
      "client_2: Loss = 0.8373, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5052981376647949\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.5148031115531921\n",
      "After round 6, train_loss = 0.5053, dev_loss = 0.5148, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 7 ...\n",
      "client_0: Loss = 0.4719, Accuracy = 0.8754\n",
      "client_1: Loss = 0.4478, Accuracy = 0.8824\n",
      "client_2: Loss = 0.8225, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5050420165061951\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.5146848559379578\n",
      "After round 7, train_loss = 0.505, dev_loss = 0.5147, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 8 ...\n",
      "client_0: Loss = 0.4622, Accuracy = 0.8773\n",
      "client_1: Loss = 0.4576, Accuracy = 0.8815\n",
      "client_2: Loss = 0.8287, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5038726329803467\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.5134056210517883\n",
      "After round 8, train_loss = 0.5039, dev_loss = 0.5134, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 9 ...\n",
      "client_0: Loss = 0.465, Accuracy = 0.8769\n",
      "client_1: Loss = 0.4586, Accuracy = 0.8825\n",
      "client_2: Loss = 0.8201, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5049070119857788\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.5147351622581482\n",
      "After round 9, train_loss = 0.5049, dev_loss = 0.5147, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n",
      "Start Round 10 ...\n",
      "client_0: Loss = 0.466, Accuracy = 0.8759\n",
      "client_1: Loss = 0.4503, Accuracy = 0.8826\n",
      "client_2: Loss = 0.8274, Accuracy = 0.7264\n",
      "evaluate_losses.,shape (346,)\n",
      "evaluate_avg_loss 0.5024687647819519\n",
      "evaluate_losses.,shape (87,)\n",
      "evaluate_avg_loss 0.5119722485542297\n",
      "After round 10, train_loss = 0.5025, dev_loss = 0.512, ,train_acc=0.8652, dev_acc = 0.8609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_net = to_device(DNNModel(), device)\n",
    "history = []\n",
    "j=0\n",
    "for i in range(rounds):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = global_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for index, client in enumerate(clients):\n",
    "    \n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "    global_net.apply_parameters(new_parameters)\n",
    "    \n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, dev_loss = {}, ,train_acc={}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n",
    "            round(dev_loss, 4),round(train_acc, 4) ,round(dev_acc, 4)))\n",
    "    history.append((train_loss, dev_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d28d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd0f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0113f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ceb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324abe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5286aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17845f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
