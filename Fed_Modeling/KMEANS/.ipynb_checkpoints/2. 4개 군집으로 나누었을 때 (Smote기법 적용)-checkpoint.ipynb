{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0449aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99da33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/군집화 라벨링_7days.csv\",encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "085b239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df.drop(columns=['before_height','before_weight','before_waist','after_height','after_weight','after_waist','waist_bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f52bd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "collect_datetime    0\n",
       "gender              0\n",
       "grade               0\n",
       "height              0\n",
       "weight              0\n",
       "step_count          0\n",
       "burned calory       0\n",
       "eat_calory          0\n",
       "Sleep_time          0\n",
       "bmi                 0\n",
       "class_num           0\n",
       "labels              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b83b3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>collect_datetime</th>\n",
       "      <th>gender</th>\n",
       "      <th>grade</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>step_count</th>\n",
       "      <th>burned calory</th>\n",
       "      <th>eat_calory</th>\n",
       "      <th>Sleep_time</th>\n",
       "      <th>bmi</th>\n",
       "      <th>class_num</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>45.15</td>\n",
       "      <td>72.0</td>\n",
       "      <td>8.877</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.612673</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>45.15</td>\n",
       "      <td>72.0</td>\n",
       "      <td>8.877</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.612673</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>45.15</td>\n",
       "      <td>72.0</td>\n",
       "      <td>8.877</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.612673</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>45.15</td>\n",
       "      <td>72.0</td>\n",
       "      <td>8.877</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.612673</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#AAGQKY</td>\n",
       "      <td>2022-07-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>45.15</td>\n",
       "      <td>72.0</td>\n",
       "      <td>8.877</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.612673</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57220</th>\n",
       "      <td>#ZZLSSL</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2103.0</td>\n",
       "      <td>38.812</td>\n",
       "      <td>521.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>29.048656</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57221</th>\n",
       "      <td>#ZZLSSL</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2103.0</td>\n",
       "      <td>38.812</td>\n",
       "      <td>521.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>29.048656</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57222</th>\n",
       "      <td>#ZZLSSL</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2103.0</td>\n",
       "      <td>38.812</td>\n",
       "      <td>521.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>29.048656</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57223</th>\n",
       "      <td>#ZZLSSL</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2103.0</td>\n",
       "      <td>38.812</td>\n",
       "      <td>521.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>29.048656</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57224</th>\n",
       "      <td>#ZZLSSL</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2103.0</td>\n",
       "      <td>38.812</td>\n",
       "      <td>521.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>29.048656</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57225 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID collect_datetime  gender  grade  height  weight  step_count  \\\n",
       "0      #AAGQKY       2022-07-21     2.0    4.0   148.0   45.15        72.0   \n",
       "1      #AAGQKY       2022-07-22     2.0    4.0   148.0   45.15        72.0   \n",
       "2      #AAGQKY       2022-07-23     2.0    4.0   148.0   45.15        72.0   \n",
       "3      #AAGQKY       2022-07-24     2.0    4.0   148.0   45.15        72.0   \n",
       "4      #AAGQKY       2022-07-25     2.0    4.0   148.0   45.15        72.0   \n",
       "...        ...              ...     ...    ...     ...     ...         ...   \n",
       "57220  #ZZLSSL       2023-01-07     1.0    6.0   153.0   68.00      2103.0   \n",
       "57221  #ZZLSSL       2023-01-08     1.0    6.0   153.0   68.00      2103.0   \n",
       "57222  #ZZLSSL       2023-01-09     1.0    6.0   153.0   68.00      2103.0   \n",
       "57223  #ZZLSSL       2023-01-10     1.0    6.0   153.0   68.00      2103.0   \n",
       "57224  #ZZLSSL       2023-01-11     1.0    6.0   153.0   68.00      2103.0   \n",
       "\n",
       "       burned calory  eat_calory  Sleep_time        bmi  class_num  labels  \n",
       "0              8.877      2000.0        12.0  20.612673          2       0  \n",
       "1              8.877      2000.0        12.0  20.612673          2       0  \n",
       "2              8.877      2000.0        12.0  20.612673          2       0  \n",
       "3              8.877      2000.0        12.0  20.612673          2       0  \n",
       "4              8.877      2000.0        12.0  20.612673          2       0  \n",
       "...              ...         ...         ...        ...        ...     ...  \n",
       "57220         38.812       521.0       442.0  29.048656          3       0  \n",
       "57221         38.812       521.0       442.0  29.048656          3       0  \n",
       "57222         38.812       521.0       442.0  29.048656          3       0  \n",
       "57223         38.812       521.0       442.0  29.048656          3       0  \n",
       "57224         38.812       521.0       442.0  29.048656          3       0  \n",
       "\n",
       "[57225 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b37a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 3], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop.class_num.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb6908",
   "metadata": {},
   "source": [
    "# 군집화 라벨별로 N개 그룹으로 데이터 세트 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982321d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0=df_drop[df_drop['class_num']==0]\n",
    "df_1=df_drop[df_drop['class_num']==1]\n",
    "df_2=df_drop[df_drop['class_num']==2]\n",
    "df_3=df_drop[df_drop['class_num']==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a0d21",
   "metadata": {},
   "source": [
    "# 데이터세트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4da2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,[2,3,4,5,6,7,8,9,10]]\n",
    "y=df.iloc[:,-1].to_frame()\n",
    "\n",
    "X_0=df_0.iloc[:,[2,3,4,5,6,7,8,9,10]]\n",
    "y_0=df_0.iloc[:,-1].to_frame()\n",
    "\n",
    "X_1=df_1.iloc[:,[2,3,4,5,6,7,8,9,10]]\n",
    "y_1=df_1.iloc[:,-1].to_frame()\n",
    "\n",
    "X_2=df_2.iloc[:,[2,3,4,5,6,7,8,9,10]]\n",
    "y_2=df_2.iloc[:,-1].to_frame()\n",
    "\n",
    "X_3=df_3.iloc[:,[2,3,4,5,6,7,8,9,10]]\n",
    "y_3=df_3.iloc[:,-1].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799b35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "scaler= StandardScaler()\n",
    "scaler_0 = StandardScaler()\n",
    "scaler_1 = StandardScaler()\n",
    "scaler_2 = StandardScaler()\n",
    "scaler_3 = StandardScaler()\n",
    "\n",
    "X_scaler = scaler_0.fit_transform(X)\n",
    "X_scaler_0 = scaler_0.fit_transform(X_0)\n",
    "X_scaler_1=scaler_1.fit_transform(X_1)\n",
    "X_scaler_2= scaler_2.fit_transform(X_2)\n",
    "X_scaler_3= scaler_3.fit_transform(X_3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#smote = SMOTE(random_state=0)\n",
    "# X_train_over,y_train_over = smote.fit_resample(X_scaler,y)\n",
    "# print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_scaler.shape, y.shape)\n",
    "# print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)\n",
    "# print('SMOTE 적용 전 레이블 값 분포: \\n', pd.Series(y).value_counts())\n",
    "# print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_train_over).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328b57b",
   "metadata": {},
   "source": [
    "# 데이터스케일링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c5c2e",
   "metadata": {},
   "source": [
    "# Train, Test dataset 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12bcbf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(X_scaler_0, y_0, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fcee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_scaler_1, y_1, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1420c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_scaler_2, y_2, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861197b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_scaler_3, y_3, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b927e",
   "metadata": {},
   "source": [
    "## Global train 데이터 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b526a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_0, X_train_1,X_train_2,X_train_3), axis=0)\n",
    "y_train = np.concatenate((y_train_0, y_train_1,y_train_2,y_train_3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eefc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate((X_test_0, X_test_1,X_test_2,X_test_3), axis=0)\n",
    "y_test = np.concatenate((y_test_0, y_test_1,y_test_2,y_test_3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd3c0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "train_dataset=TensorDataset(X_train, y_train)\n",
    "test_dataset=TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfebb51",
   "metadata": {},
   "source": [
    "## Client train 데이터 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01841ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0 = torch.FloatTensor(X_train_0)\n",
    "X_test_0 = torch.FloatTensor(X_test_0)\n",
    "y_train_0 = torch.LongTensor(y_train_0.to_numpy())\n",
    "y_test_0 = torch.LongTensor(y_test_0.to_numpy())\n",
    "\n",
    "train_dataset_0=TensorDataset(X_train_0, y_train_0)\n",
    "test_dataset_0=TensorDataset(X_test_0, y_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d56344",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = torch.FloatTensor(X_train_1)\n",
    "X_test_1 = torch.FloatTensor(X_test_1)\n",
    "y_train_1 = torch.LongTensor(y_train_1.to_numpy())\n",
    "y_test_1 = torch.LongTensor(y_test_1.to_numpy())\n",
    "\n",
    "train_dataset_1=TensorDataset(X_train_1, y_train_1)\n",
    "test_dataset_1=TensorDataset(X_test_1, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96536afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = torch.FloatTensor(X_train_2)\n",
    "X_test_2 = torch.FloatTensor(X_test_2)\n",
    "y_train_2 = torch.LongTensor(y_train_2.to_numpy())\n",
    "y_test_2 = torch.LongTensor(y_test_2.to_numpy())\n",
    "\n",
    "train_dataset_2=TensorDataset(X_train_2, y_train_2)\n",
    "test_dataset_2=TensorDataset(X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89dd4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = torch.FloatTensor(X_train_3)\n",
    "X_test_3 = torch.FloatTensor(X_test_3)\n",
    "y_train_3 = torch.LongTensor(y_train_3.to_numpy())\n",
    "y_test_3 = torch.LongTensor(y_test_3.to_numpy())\n",
    "\n",
    "train_dataset_3=TensorDataset(X_train_3, y_train_3)\n",
    "test_dataset_3=TensorDataset(X_test_3, y_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f4ba7",
   "metadata": {},
   "source": [
    "# 파라미터 값 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "341e1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size=len(train_dataset)\n",
    "\n",
    "total_train_size_0 = len(train_dataset_0)\n",
    "total_test_size_0 = len(test_dataset_0)\n",
    "\n",
    "total_train_size_1 = len(train_dataset_1)\n",
    "total_test_size_1 = len(test_dataset_1)\n",
    "\n",
    "total_train_size_2 = len(train_dataset_2)\n",
    "total_test_size_2 = len(test_dataset_2)\n",
    "\n",
    "total_train_size_3 = len(train_dataset_3)\n",
    "total_test_size_3 = len(test_dataset_3)\n",
    "\n",
    "#total_train_size=[total_train_size_0,total_train_size_1,total_train_size_2,total_train_size_3]\n",
    "#otal_test_size=[total_test_size_0,total_test_size_1,total_test_size_2,total_test_size_3]\n",
    "\n",
    "#total_dev_size = len(dev_dataset)\n",
    "\n",
    "classes = 3\n",
    "input_dim = 9\n",
    "\n",
    "num_clients = 4\n",
    "rounds = 10\n",
    "batch_size = 5000\n",
    "epochs_per_client = 30\n",
    "learning_rate = 2e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec952669",
   "metadata": {},
   "source": [
    "# GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31f30353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef982036",
   "metadata": {},
   "source": [
    "# 딥러닝 모델 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26f5959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 128)\n",
    "        self.hidden_layer1 = nn.Linear(128, 256)\n",
    "        self.hidden_layer2 = nn.Linear(256, 128)\n",
    "        self.output_layer   = nn.Linear(128,3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.track_layers = {'hidden_layer1': self.hidden_layer1, 'hidden_layer2': self.hidden_layer2, 'output_layer': self.output_layer}\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  self.relu(self.input_layer(x))\n",
    "        out =  self.relu(self.hidden_layer1(out))\n",
    "        out =  self.relu(self.hidden_layer2(out))\n",
    "        out =  self.output_layer(out)\n",
    "        return out \n",
    "    \n",
    "    def get_track_layers(self):\n",
    "        return self.track_layers\n",
    "    \n",
    "    def apply_parameters(self, parameters_dict):\n",
    "        with torch.no_grad():\n",
    "            for layer_name in parameters_dict:\n",
    "                self.track_layers[layer_name].weight.data *= 0\n",
    "                self.track_layers[layer_name].bias.data *= 0\n",
    "                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n",
    "                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        parameters_dict = dict()\n",
    "        for layer_name in self.track_layers:\n",
    "            parameters_dict[layer_name] = {\n",
    "                'weight': self.track_layers[layer_name].weight.data, \n",
    "                'bias': self.track_layers[layer_name].bias.data\n",
    "            }\n",
    "        return parameters_dict\n",
    "    \n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        \n",
    "        data, labels = batch\n",
    "        outputs= self(data)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels.squeeze(dim=-1))\n",
    "        accuracy = self.batch_accuracy(outputs, labels.squeeze(dim=-1))\n",
    "        return (loss, accuracy)\n",
    "    \n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        print(\"evaluate_losses.,shape\",np.array(losses).shape)\n",
    "        print(\"evaluate_avg_loss\",avg_loss)\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea487b45",
   "metadata": {},
   "source": [
    "# 클라이언트 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1b2eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "    \n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(DNNModel(), device)\n",
    "        net.apply_parameters(parameters_dict)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n",
    "        return net.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05981a7",
   "metadata": {},
   "source": [
    "# 클라이언트 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58b43eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_datasets = random_split(train_dataset_0, [min(i + examples_per_client,total_train_size) - i for i in range(0, total_train_size, examples_per_client)])\n",
    "client_datasets=[train_dataset_0,train_dataset_1,train_dataset_2,train_dataset_3]\n",
    "\n",
    "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f85181",
   "metadata": {},
   "source": [
    "# 중앙 서버 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bb29937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Round 1 ...\n",
      "client_0: Loss = 0.7398, Accuracy = 0.748\n",
      "client_1: Loss = 0.7491, Accuracy = 0.7592\n",
      "client_2: Loss = 0.8019, Accuracy = 0.7078\n",
      "client_3: Loss = 0.5359, Accuracy = 0.8484\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.7370970249176025\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7340423464775085\n",
      "After round 1, train_loss = 0.7371, dev_loss = 0.734, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 2 ...\n",
      "client_0: Loss = 0.7357, Accuracy = 0.7454\n",
      "client_1: Loss = 0.6996, Accuracy = 0.7577\n",
      "client_2: Loss = 0.7976, Accuracy = 0.6955\n",
      "client_3: Loss = 0.5372, Accuracy = 0.8466\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.7205634713172913\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7169761061668396\n",
      "After round 2, train_loss = 0.7206, dev_loss = 0.717, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 3 ...\n",
      "client_0: Loss = 0.7359, Accuracy = 0.7456\n",
      "client_1: Loss = 0.691, Accuracy = 0.7591\n",
      "client_2: Loss = 0.7649, Accuracy = 0.7119\n",
      "client_3: Loss = 0.5344, Accuracy = 0.8452\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.7159649729728699\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7118927240371704\n",
      "After round 3, train_loss = 0.716, dev_loss = 0.7119, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 4 ...\n",
      "client_0: Loss = 0.7287, Accuracy = 0.7492\n",
      "client_1: Loss = 0.6955, Accuracy = 0.7604\n",
      "client_2: Loss = 0.8169, Accuracy = 0.6668\n",
      "client_3: Loss = 0.5176, Accuracy = 0.8496\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.714633047580719\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7104847431182861\n",
      "After round 4, train_loss = 0.7146, dev_loss = 0.7105, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 5 ...\n",
      "client_0: Loss = 0.7294, Accuracy = 0.7465\n",
      "client_1: Loss = 0.6993, Accuracy = 0.7583\n",
      "client_2: Loss = 0.7813, Accuracy = 0.7037\n",
      "client_3: Loss = 0.5248, Accuracy = 0.8466\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.7129350304603577\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7086594700813293\n",
      "After round 5, train_loss = 0.7129, dev_loss = 0.7087, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 6 ...\n",
      "client_0: Loss = 0.734, Accuracy = 0.7453\n",
      "client_1: Loss = 0.6975, Accuracy = 0.7567\n",
      "client_2: Loss = 0.7862, Accuracy = 0.6914\n",
      "client_3: Loss = 0.5093, Accuracy = 0.8513\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.7116011381149292\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7072207927703857\n",
      "After round 6, train_loss = 0.7116, dev_loss = 0.7072, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 7 ...\n",
      "client_0: Loss = 0.7306, Accuracy = 0.7457\n",
      "client_1: Loss = 0.6907, Accuracy = 0.757\n",
      "client_2: Loss = 0.7985, Accuracy = 0.6791\n",
      "client_3: Loss = 0.5334, Accuracy = 0.8419\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.710585355758667\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7062236070632935\n",
      "After round 7, train_loss = 0.7106, dev_loss = 0.7062, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 8 ...\n",
      "client_0: Loss = 0.7274, Accuracy = 0.7462\n",
      "client_1: Loss = 0.698, Accuracy = 0.7573\n",
      "client_2: Loss = 0.8285, Accuracy = 0.6709\n",
      "client_3: Loss = 0.5158, Accuracy = 0.8479\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.7096251845359802\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7052416801452637\n",
      "After round 8, train_loss = 0.7096, dev_loss = 0.7052, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 9 ...\n",
      "client_0: Loss = 0.7218, Accuracy = 0.7489\n",
      "client_1: Loss = 0.6986, Accuracy = 0.7594\n",
      "client_2: Loss = 0.7714, Accuracy = 0.7037\n",
      "client_3: Loss = 0.5225, Accuracy = 0.8454\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.7087000608444214\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7043832540512085\n",
      "After round 9, train_loss = 0.7087, dev_loss = 0.7044, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n",
      "Start Round 10 ...\n",
      "client_0: Loss = 0.7299, Accuracy = 0.7464\n",
      "client_1: Loss = 0.684, Accuracy = 0.7588\n",
      "client_2: Loss = 0.7681, Accuracy = 0.6914\n",
      "client_3: Loss = 0.5174, Accuracy = 0.8459\n",
      "evaluate_losses.,shape (358,)\n",
      "evaluate_avg_loss 0.7087379097938538\n",
      "evaluate_losses.,shape (90,)\n",
      "evaluate_avg_loss 0.7044517993927002\n",
      "After round 10, train_loss = 0.7087, dev_loss = 0.7045, ,train_acc=0.7601, dev_acc = 0.7619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_net = to_device(DNNModel(), device)\n",
    "history = []\n",
    "j=0\n",
    "for i in range(rounds):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = global_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for index, client in enumerate(clients):\n",
    "    \n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "    global_net.apply_parameters(new_parameters)\n",
    "    \n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, dev_loss = {}, ,train_acc={}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n",
    "            round(dev_loss, 4),round(train_acc, 4) ,round(dev_acc, 4)))\n",
    "    history.append((train_loss, dev_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffcfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2e695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1915ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8943a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7741e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b776fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a4365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fe48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9ba52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318af9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
